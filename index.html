<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yiyi Liao


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.useso.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%79%69%79%69.%6C%69%61%6F@%7A%6A%75.%65%64%75.%63%6E"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/yiyiliao" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>















        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Yiyi Liao
    </h1>
     <p class="desc">Zhejiang University</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/prof_pic-480x659.png" srcset="    /assets/resized/prof_pic-480x659.png 480w,/assets/img/prof_pic.png 648w">

      
      
    </div>
    

    <div class="clearfix">
      <p>I am a tenure-track research professor in <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a>. Before that, I was a Postdoc in <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group</a>, a part of the University of Tübingen and the MPI for Intelligent Systems, working with <a href="https://avg.is.tuebingen.mpg.de/person/ageiger" target="_blank" rel="noopener noreferrer">Prof. Andreas Geiger</a>. I received my Ph.D. in Control Science and Engineering from <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in June 2018 and the B.S. degree from <a href="https://en.wikipedia.org/wiki/Xi%27an_Jiaotong_University" target="_blank" rel="noopener noreferrer">Xi’an Jiaotong University</a> in 2013.</p>

<p>My research interest lies in 3D computer vision, including 3D scene understanding, 3D reconstruction, depth estimation and 3D controllable image synthesis.</p>

<p>For prospective students interested in computer vision, feel free to contact me via <a href="mailto:yiyi.liao@zju.edu.cn">email</a>!</p>


      
    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless" style="width: 100%">
        <colgroup>
           <col span="1" style="width: 15%;">
           <col span="1" style="width: 85%;">
        </colgroup>
      <!-- Put <thead>, <tbody>, and <tr>'s here! -->
      <tbody>
        
        
          <tr>
            <th scope="row">Aug 5, 2022</th>
            <td>
              
                Our <a href="https://fuxiao0719.github.io/projects/panopticnerf/" target="_blank" rel="noopener noreferrer">Panoptic NeRF</a> is accepted to 3DV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 5, 2022</th>
            <td>
              
                Our work on <a href="https://arxiv.org/abs/2203.13572" target="_blank" rel="noopener noreferrer">category-level object pose estimation</a> leveraging 3D-aware generative models is accepted to ECCV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 6, 2022</th>
            <td>
              
                I was invited to give a talk at the <a href="https://sites.google.com/view/3d-dlad-v4-iv2022/" target="_blank" rel="noopener noreferrer">3D-DLAD workshop</a> at IV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 4, 2022</th>
            <td>
              
                Our KITTI-360 is accepted to TPAMI and we released all benchmarks! Check out our <a href="https://autonomousvision.github.io/kitti-360/" target="_blank" rel="noopener noreferrer">blog</a> for more information of the benchmarks.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Apr 15, 2022</th>
            <td>
              
                I will serve as an Area Chair for 3DV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 29, 2021</th>
            <td>
              
                Two papers (1 <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS.pdf" target="_blank" rel="noopener noreferrer">oral</a>, 1 <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" target="_blank" rel="noopener noreferrer">poster</a>) are accepted to NeurIPS 2021.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 24, 2021</th>
            <td>
              
                Our work <a href="https://github.com/fabiotosi92/SMD-Nets" target="_blank" rel="noopener noreferrer">SMD-Nets</a> was featured on the <a href="https://www.rsipvision.com/CVPR2021-Wednesday/6/" target="_blank" rel="noopener noreferrer">CVPR Daily</a> and the <a href="https://www.rsipvision.com/ComputerVisionNews-2021July/30/" target="_blank" rel="noopener noreferrer">BEST OF CVPR of Computer Vision News</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 2, 2021</th>
            <td>
              
                I will be joining <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> as a tenure-track assistant professor this September!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">May 31, 2021</th>
            <td>
              
                I will serve as an Area Chair for BMVC 2021.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">May 20, 2021</th>
            <td>
              
                I was acknowledged as <a href="http://cvpr2021.thecvf.com/node/184" target="_blank" rel="noopener noreferrer">Outstanding Reviewer</a> at CVPR 2021.

              
            </td>
          </tr>
        
      </tbody>
      <!--
      <tr>
        <td class="date"></td>
      <td class="announcement">
        <a class="all-news" href="https://yiyiliao.github.io/news/">All older news items</a>
      </td>
      </tr>
      -->
      </table>
    </div>
  
</div>

    

    <div class="publications">
    <h2>selected publications</h2>
    <p> Full publication list can be found on <a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
    <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author. </p>
    
    
      <h2 class="year">2022</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/3dv2022.gif" alt="Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Fu2022THREEDV" class="col-md-9">
    
      <div class="title">Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://fuxiao0719.github.io/" target="_blank" rel="noopener noreferrer">Fu, Xiao<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Shangzhan<sup>*</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chen, Tianrun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lu, Yichong,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://lanyunzhu.site/" target="_blank" rel="noopener noreferrer">Zhu, Lanyun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the International Conf. on 3D Vision (3DV)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2203.15224.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/eccv2022.jpg" alt="A Visual Navigation Perspective for Category-Level Object Pose Estimation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Guo2022ECCV" class="col-md-9">
    
      <div class="title">A Visual Navigation Perspective for Category-Level Object Pose Estimation</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.linkedin.com/in/jiaxin-guo-043a6b1b8" target="_blank" rel="noopener noreferrer">Guo, Jiaxin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://billyzhonghk.wixsite.com/billyzhong" target="_blank" rel="noopener noreferrer">Zhong, Fangxun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cse.zju.edu.cn/english/redir.php?catalog_id=1113878&amp;object_id=1115898" target="_blank" rel="noopener noreferrer">Xiong, Rong</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.mrc-cuhk.com/people/prof-yun-hui-liu" target="_blank" rel="noopener noreferrer">Liu, Yunhui</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the European Conf. on Computer Vision (ECCV)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2203.13572.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables, \eg, pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/kitti360.png" alt="KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liao2022TPAMI" class="col-md-9">
    
      <div class="title">KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cbsr.ia.ac.cn/users/ynyu/" target="_blank" rel="noopener noreferrer">Xie, Jun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2022PAMI.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
      <a href="https://autonomousvision.github.io/kitti-360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="http://www.cvlibs.net/datasets/kitti-360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over  150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today’s grand challenges: the development of fully autonomous self-driving systems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2021</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
      <video width="160" height="120" muted autoplay loop>
         <source src="/assets/teaser/sap_teaser.mp4" type="video/mp4">
         Your browser does not support the video tag.
      </source></video>
    
  </div>

  <div id="Peng2021NEURIPS" class="col-md-9">
    
      <div class="title">Shape As Points: A Differentiable Poisson Solver</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsongyou.github.io/" target="_blank" rel="noopener noreferrer">Peng, Songyou</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.maxjiang.ml/" target="_blank" rel="noopener noreferrer">Jiang, Chiyu<sup>♯</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.inf.ethz.ch/pomarc/" target="_blank" rel="noopener noreferrer">Pollefeys, Marc</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/shape_as_points" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/frequency_bias.gif" alt="On the Frequency Bias of Generative Models" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2021NEURIPS" class="col-md-9">
    
      <div class="title">On the Frequency Bias of Generative Models</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/frequency_bias" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The key objective of Generative Adversarial Networks (GANs) is to generate new data
  with the same statistics as the provided training data. However, multiple recent works show that
  state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an
  elevated amount of high frequencies in the spectral statistics which makes it straightforward to
  distinguish real and generated images. Explanations for this phenomenon are controversial: While
  most works attribute the artifacts to the generator, other works point to the discriminator. We
  take a sober look at those explanations and provide insights on what makes proposed measures
  against high-frequency artifacts effective. To achieve this, we first independently assess the
  architectures of both the generator and discriminator and investigate if they exhibit a frequency
  bias that makes learning the distribution of high-frequency content particularly problematic.
  Based on these experiments, we make the following four observations: 1) Different upsampling
  operations bias the generator towards different spectral properties. 2) Checkerboard artifacts
  introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able
  to compensate for these artifacts. 3) The discriminator does not struggle with detecting high
  frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling
  operations in the discriminator can impair the quality of the training signal it provides. In
  light of these findings, we analyze proposed measures against high-frequency artifacts in
  state-of-the-art GAN training but find that none of the existing approaches can fully resolve
  spectral artifacts yet. Our results suggest that there is great potential in improving the
  discriminator and that this could be key to match the distribution of the training data more
  closely.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/iccv2021.png" alt="KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs" style="width: 100%;">
    </div>
    
  </div>

  <div id="Reiser2021ICCV" class="col-md-9">
    
      <div class="title">KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/creiser" target="_blank" rel="noopener noreferrer">Reiser, Christian</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsongyou.github.io/" target="_blank" rel="noopener noreferrer">Peng, Songyou</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Reiser2021ICCV.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Reiser2021ICCV_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/kilonerf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://creiser.github.io/kilonerf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that significant speed-ups are possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by two orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2021.png" alt="SMD-Nets: Stereo Mixture Density Networks" style="width: 100%;">
    </div>
    
  </div>

  <div id="Tosi2021CVPR" class="col-md-9">
    
      <div class="title">SMD-Nets: Stereo Mixture Density Networks</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://vision.disi.unibo.it/~ftosi/" target="_blank" rel="noopener noreferrer">Tosi, Fabio</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/cschmitt" target="_blank" rel="noopener noreferrer">Schmitt, Carolin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Tosi2021CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Tosi2021CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/smdnets/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/fabiotosi92/SMD-Nets" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite stereo matching accuracy has greatly improved by deep learning in the last few years, recovering sharp boundaries and high-resolution outputs efficiently remains challenging. In this paper, we propose Stereo Mixture Density Networks (SMD-Nets), a simple yet effective learning framework compatible with a wide class of 2D and 3D architectures which ameliorates both issues. Specifically, we exploit bimodal mixture densities as output representation and show that this allows for sharp and precise disparity estimates near discontinuities while explicitly modeling the aleatoric uncertainty inherent in the observations. Moreover, we formulate disparity estimation as a continuous problem in the image domain, allowing our model to query disparities at arbitrary spatial precision. We carry out comprehensive experiments on a new high-resolution and highly realistic synthetic stereo dataset, consisting of stereo pairs at 8Mpx resolution, as well as on real-world stereo datasets. Our experiments demonstrate increased depth accuracy near object boundaries and prediction of ultra high-resolution disparity maps on standard GPUs. We demonstrate the flexibility of our technique by improving the performance of a variety of stereo backbones.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tip2021.png" alt="Learning Steering Kernels for Guided Depth Completion" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liu2021TIP" class="col-md-9">
    
      <div class="title">Learning Steering Kernels for Guided Depth Completion</div>
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://april.zju.edu.cn/team/lina-liu/" target="_blank" rel="noopener noreferrer">Liu, Lina</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong<sup>♯</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on  Image Processing (TIP)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liu2021TIP.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/LINA-lln/Steering-KernelNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper addresses the guided depth completion
        task in which the goal is to predict a dense depth map given a
        guidance RGB image and sparse depth measurements. Recent
        advances on this problem nurture hopes that one day we can
        acquire accurate and dense depth at a very low cost. A major
        challenge of guided depth completion is to effectively make use
        of extremely sparse measurements, e.g., measurements covering
        less than 1% of the image pixels. In this paper, we propose
        a fully differentiable model that avoids convolving on sparse
        tensors by jointly learning depth interpolation and refinement.
        More specifically, we propose a differentiable kernel regression
        layer that interpolates the sparse depth measurements via learned
        kernels. We further refine the interpolated depth map using
        a residual depth refinement layer which leads to improved
        performance compared to learning absolute depth prediction
        using a vanilla network. We provide experimental evidence that
        our differentiable kernel regression layer not only enables end-to-end training from very sparse measurements using standard
        convolutional network architectures, but also leads to better
        depth interpolation results compared to existing heuristically
        motivated methods. We demonstrate that our method outperforms many state-of-the-art guided depth completion techniques
        on both NYUv2 and KITTI. We further show the generalization
        ability of our method with respect to the density and spatial
        statistics of the sparse depth measurements.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2020</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/graf.gif" alt="GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2020NEURIPS" class="col-md-9">
    
      <div class="title">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/graf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/graf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=akQf7WaCOHo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While 2D generative adversarial networks have enabled high-resolution image synthesis,
  they largely lack an understanding of the 3D world and the image formation process. Thus, they do
  not provide precise control over camera viewpoint or object pose. To address this problem, several
  recent approaches leverage intermediate voxel-based representations in combination with differentiable
  rendering. However, existing methods either produce low image resolution or fall short in disentangling
  camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper,
  we propose a generative model for radiance fields which have recently proven successful for novel view
  synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not
  confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene
  properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a
  multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training
  our model from unposed 2D images alone. We systematically analyze our approach on several challenging
  synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful
  representation for generative image synthesis, leading to 3D consistent models that render with high
  fidelity.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/controllableGAN.gif" alt="Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liao2020CVPR" class="col-md-9">
    
      <div class="title">Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/lmescheder" target="_blank" rel="noopener noreferrer">Mescheder, Lars</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/controllable-gan/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/controllable_image_synthesis" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=ygQCgGC0Lm8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, Generative Adversarial Networks have achieved impressive results
   in photorealistic image synthesis. This progress nurtures hopes that one day the classical
   rendering pipeline can be replaced by efficient models that are learned directly from images.
   However, current image synthesis models operate in the 2D domain where disentangling 3D properties
   such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable
   and controllable representation. Our key hypothesis is that the image generation process should
   be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional.
   We define the new task of 3D controllable image synthesis and propose an approach for solving it
   by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to
   disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw
   images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt.
   changes in viewpoint or object pose. We further evaluate various 3D representations in terms of
   their usefulness for this challenging task.
   </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2019</h2>
      <ol class="bibliography"><li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2019.png" alt="Connecting the Dots: Learning Representations for Active Monocular Depth Estimation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Riegler2019CVPR" class="col-md-9">
    
      <div class="title">Connecting the Dots: Learning Representations for Active Monocular Depth Estimation</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://griegler.github.io/" target="_blank" rel="noopener noreferrer">Riegler, Gernot<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://donnessime.github.io/" target="_blank" rel="noopener noreferrer">Donné, Simon</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://vladlen.info/" target="_blank" rel="noopener noreferrer">Koltun, Vladlen</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/connecting_the_dots" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a technique for depth estimation with a monocular structured-light camera, \ie, a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufficient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2018</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2018.png" alt="Deep Marching Cubes: Learning Explicit Surface Representations" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2018cvpr" class="col-md-9">
    
      <div class="title">Deep Marching Cubes: Learning Explicit Surface Representations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://donnessime.github.io/" target="_blank" rel="noopener noreferrer">Donné, Simon</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="http://www.cvlibs.net/redirect.php?site=deep_marching_cubes" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (eg, TSDF) from which 3D surface meshes must be extracted in a post-processing step (eg, via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object’s inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/gcpr2018.png" alt="On the Integration of Optical Flow and Action Recognition" style="width: 100%;">
    </div>
    
  </div>

  <div id="Sevilla:GCPR:2018" class="col-md-9">
    
      <div class="title">On the Integration of Optical Flow and Action Recognition</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.cs.umass.edu/~lsevilla/" target="_blank" rel="noopener noreferrer">Sevilla-Lara, Laura</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ps.is.tuebingen.mpg.de/person/fguney" target="_blank" rel="noopener noreferrer">Guney, Fatma</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://varunjampani.github.io/" target="_blank" rel="noopener noreferrer">Jampani, Varun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://ps.is.tuebingen.mpg.de/person/black" target="_blank" rel="noopener noreferrer">Black, Michael</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In German Conference on Pattern Recognition (GCPR)</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Sevilla-Lara2018GCPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most of the top performing action recognition methods use optical flow as a black box input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2017</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tip2017.png" alt="Graph Regularized Auto-encoders for Image Representation" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017graph" class="col-md-9">
    
      <div class="title">Graph Regularized Auto-encoders for Image Representation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on  Image Processing (TIP)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7556994" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image representation has been intensively explored in the domain of computer vision for its significant influence on the relative tasks such as image clustering and classification. It is valuable to learn a low-dimensional representation of an image which preserves its inherent information from the original image space. At the perspective of manifold learning, this is implemented with the local invariant idea to capture the intrinsic low-dimensional manifold embedded in the high-dimensional input space. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). With the graph regularization, the proposed method preserves the local connectivity from the original image space to the representation space, while the stacked auto-encoders provide explicit encoding model for fast inference and powerful expressive capacity for complex modeling. Theoretical analysis shows that the graph regularizer penalizes the weighted Frobenius norm of the Jacobian matrix of the encoder mapping, where the weight matrix captures the local property in the input space. Furthermore, the underlying effects on the hidden representation space are revealed, providing insightful explanation to the advantage of the proposed method. Finally, the experimental results on both clustering and classification tasks demonstrate the effectiveness of our GAE as well as the correctness of the proposed theoretical analysis, and it also suggests that GAE is a superior solution to the current deep representation learning techniques comparing with variant auto-encoders and existing local invariant methods.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tcds2017.png" alt="Place Classification with a Graph Regularized Deep Neural Network" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017place" class="col-md-9">
    
      <div class="title">Place Classification with a Graph Regularized Deep Neural Network</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://au.linkedin.com/in/lei-shi-342b152b" target="_blank" rel="noopener noreferrer">Shi, Lei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Cognitive and Developmental Systems (TCDS)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7501830" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. In recent years, there is a high exploitation of artificial intelligence algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. First, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Second, each layer of data are fed into a deep neural network for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effectiveness of our end-to-end place classification framework in which both the multilayer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/icra2017.png" alt="Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017parse" class="col-md-9">
    
      <div class="title">Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=F2e_jZMAAAAJ" target="_blank" rel="noopener noreferrer">Huang, Lichao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cbsr.ia.ac.cn/users/ynyu/" target="_blank" rel="noopener noreferrer">Yu, Yinan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1611.02174.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Many standard robotic platforms are equipped with at least a fixed 2D laser range finder and a monocular camera. Although those platforms do not have sensors for 3D depth sensing capability, knowledge of depth is an essential part in many robotics activities. Therefore, recently, there is an increasing interest in depth estimation using monocular images. As this task is inherently ambiguous, the data-driven estimated depth might be unreliable in robotics applications. In this paper, we have attempted to improve the precision of monocular depth estimation by introducing 2D planar observation from the remaining laser range finder without extra cost. Specifically, we construct a dense reference map from the sparse laser range data, redefining the depth estimation task as estimating the distance between the real and the reference depth. To solve the problem, we construct a novel residual of residual neural network, and tightly combine the classification and regression losses for continuous depth estimation. Experimental results suggest that our method achieves considerable promotion compared to the state-of-the-art methods on both NYUD2 and KITTI, validating the effectiveness of our method on leveraging the additional sensory information. We further demonstrate the potential usage of our method in obstacle avoidance where our methodology provides comprehensive depth information compared to the solution using monocular camera or 2D laser range finder alone.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2016</h2>
      <ol class="bibliography"><li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/icra2016.png" alt="Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2016understand" class="col-md-9">
    
      <div class="title">Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://au.linkedin.com/in/lei-shi-342b152b" target="_blank" rel="noopener noreferrer">Shi, Lei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)</em>
      
      
      
        2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1509.06470.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Scene classification is a fundamental perception task for environmental understanding in today’s robotics. In this paper, we have attempted to exploit the use of popular machine learning technique of deep learning to enhance scene understanding, particularly in robotics applications. As scene images have larger diversity than the iconic object images, it is more challenging for deep learning methods to automatically learn features from scene images with less samples. Inspired by human scene understanding based on object knowledge, we address the problem of scene classification by encouraging deep neural networks to incorporate object-level information. This is implemented with a regularization of semantic segmentation. With only 5 thousand training images, as opposed to 2.5 million images, we show the proposed deep architecture achieves superior scene classification results to the state-of-the-art on a publicly available SUN RGB-D dataset. In addition, performance of semantic segmentation, the regularizer, also reaches a new record with refinement derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset trained model to a mobile robot captured images to classify scenes in our university demonstrating the generalization ability of the proposed algorithm.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
    
    </div>

  </article>

</div>


    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2022 Yiyi  Liao.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: August 04, 2022.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
